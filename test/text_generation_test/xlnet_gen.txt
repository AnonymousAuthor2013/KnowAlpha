08/24/2019 09:24:00 - INFO - root -   Loading checkpoint from /home/LAB/zhangzy/ProjectModels/seq2seq_lm/xlnet/model_step_70000.pt
08/24/2019 09:24:09 - INFO - root -   Loading vocab from checkpoint at /home/LAB/zhangzy/ProjectModels/seq2seq_lm/xlnet/model_step_70000.pt.
[2019-08-24 09:24:09,111 INFO] Loading checkpoint from /home/LAB/zhangzy/ProjectModels/seq2seq_lm/xlnet/model_step_70000.pt
[2019-08-24 09:24:10,516 INFO] Loading vocab from checkpoint at /home/LAB/zhangzy/ProjectModels/seq2seq_lm/xlnet/model_step_70000.pt.
[2019-08-24 09:24:10,517 INFO]  * src vocab size = 32006
[2019-08-24 09:24:10,517 INFO]  * tgt vocab size = 32006
[2019-08-24 09:24:10,517 INFO] Building model...
[2019-08-24 09:24:10,837 INFO] building with extended encoder version:xlnet
[2019-08-24 09:24:10,861 INFO] loading weights file /home/LAB/zhangzy/ShareModels/xlnet/base-cased/pytorch_model.bin
[2019-08-24 09:24:15,607 INFO] initing embedding with extended encoder version:xlnet
[2019-08-24 09:24:15,607 INFO] decoder type is : transformer (transformer,0)
[2019-08-24 09:24:24,973 INFO] NMTModel(
  (encoder): OnmtXLNetEncoder(
    (encoder): XLNetModel(
      (word_embedding): Embedding(32006, 768, padding_idx=1)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
      )
      (dropout): Dropout(p=0.1)
    )
  )
  (decoder): TransformerDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(32006, 768, padding_idx=1)
        )
        (pe): PositionalEncoding(
          (dropout): Dropout(p=0.1)
        )
      )
      (word_lut): Embedding(32006, 768, padding_idx=1)
    )
    (transformer_layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax()
          (dropout): Dropout(p=0.1)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax()
          (dropout): Dropout(p=0.1)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1)
        )
        (layer_norm_1): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax()
          (dropout): Dropout(p=0.1)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax()
          (dropout): Dropout(p=0.1)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1)
        )
        (layer_norm_1): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax()
          (dropout): Dropout(p=0.1)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax()
          (dropout): Dropout(p=0.1)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1)
        )
        (layer_norm_1): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax()
          (dropout): Dropout(p=0.1)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (context_attn): MultiHeadedAttention(
          (linear_keys): Linear(in_features=768, out_features=768, bias=True)
          (linear_values): Linear(in_features=768, out_features=768, bias=True)
          (linear_query): Linear(in_features=768, out_features=768, bias=True)
          (softmax): Softmax()
          (dropout): Dropout(p=0.1)
          (final_linear): Linear(in_features=768, out_features=768, bias=True)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=768, out_features=3072, bias=True)
          (w_2): Linear(in_features=3072, out_features=768, bias=True)
          (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1)
          (relu): ReLU()
          (dropout_2): Dropout(p=0.1)
        )
        (layer_norm_1): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
        (drop): Dropout(p=0.1)
      )
    )
    (layer_norm): LayerNorm(torch.Size([768]), eps=1e-06, elementwise_affine=True)
  )
  (generator): CopyGenerator(
    (linear): Linear(in_features=768, out_features=32006, bias=True)
    (linear_copy): Linear(in_features=768, out_features=1, bias=True)
  )
)
[2019-08-24 09:24:24,975 INFO] encoder: 116722944
[2019-08-24 09:24:24,976 INFO] decoder: 87002631
[2019-08-24 09:24:24,976 INFO] * number of parameters: 203725575
[2019-08-24 09:24:24,977 INFO] builing BertAdamW
[2019-08-24 09:24:28,007 INFO] Starting training on GPU: [0]
[2019-08-24 09:24:28,008 INFO] Start training loop and validate every 10000 steps...
[2019-08-24 09:24:28,008 INFO] Loading dataset from /home/LAB/zhangzy/ProjectData/openNMT/answer_data/xlnetGen/data.train.0.pt
[2019-08-24 09:24:50,776 INFO] number of examples: 100000
/home/LAB/zhangzy/python_libs/torchtext/torchtext/data/field.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  var = torch.tensor(arr, dtype=self.dtype, device=device)
[2019-08-24 09:27:36,479 INFO] Step 50/200000; acc:  52.44; ppl: 154.24; xent: 5.04; lr: 0.00001; 3992/1200 tok/s;    188 sec
init XLNet model with 206 weights
************************************************************
word padding idx= 1
[2019-08-24 09:30:20,388 INFO] Step 100/200000; acc:  84.01; ppl:  1.58; xent: 0.46; lr: 0.00001; 4584/1286 tok/s;    352 sec
[2019-08-24 09:33:05,446 INFO] Step 150/200000; acc:  88.68; ppl:  1.37; xent: 0.31; lr: 0.00001; 4548/1379 tok/s;    517 sec
[2019-08-24 09:35:48,550 INFO] Step 200/200000; acc:  90.37; ppl:  1.30; xent: 0.27; lr: 0.00001; 4577/1342 tok/s;    681 sec
[2019-08-24 09:38:56,807 INFO] Step 250/200000; acc:  91.52; ppl:  1.28; xent: 0.24; lr: 0.00001; 4164/1253 tok/s;    869 sec
[2019-08-24 09:42:13,802 INFO] Step 300/200000; acc:  92.02; ppl:  1.28; xent: 0.25; lr: 0.00001; 3916/1135 tok/s;   1066 sec
[2019-08-24 09:45:39,445 INFO] Step 350/200000; acc:  92.98; ppl:  1.22; xent: 0.20; lr: 0.00001; 3864/1079 tok/s;   1271 sec
[2019-08-24 09:48:57,872 INFO] Step 400/200000; acc:  93.86; ppl:  1.19; xent: 0.17; lr: 0.00001; 3796/1164 tok/s;   1470 sec
[2019-08-24 09:52:24,500 INFO] Step 450/200000; acc:  94.14; ppl:  1.18; xent: 0.17; lr: 0.00001; 3808/1136 tok/s;   1676 sec
[2019-08-24 09:55:40,487 INFO] Step 500/200000; acc:  94.25; ppl:  1.18; xent: 0.16; lr: 0.00001; 3887/1098 tok/s;   1872 sec
[2019-08-24 09:59:06,283 INFO] Step 550/200000; acc:  94.81; ppl:  1.16; xent: 0.15; lr: 0.00001; 3785/1113 tok/s;   2078 sec
[2019-08-24 10:02:05,273 INFO] Step 600/200000; acc:  95.06; ppl:  1.15; xent: 0.14; lr: 0.00001; 3999/1193 tok/s;   2257 sec
[2019-08-24 10:05:28,511 INFO] Step 650/200000; acc:  95.13; ppl:  1.15; xent: 0.14; lr: 0.00001; 3824/1144 tok/s;   2461 sec
[2019-08-24 10:08:43,716 INFO] Step 700/200000; acc:  95.51; ppl:  1.13; xent: 0.13; lr: 0.00001; 3854/1194 tok/s;   2656 sec
[2019-08-24 10:12:02,039 INFO] Step 750/200000; acc:  95.40; ppl:  1.15; xent: 0.14; lr: 0.00001; 3894/1100 tok/s;   2854 sec
[2019-08-24 10:15:21,492 INFO] Step 800/200000; acc:  95.72; ppl:  1.15; xent: 0.14; lr: 0.00001; 3867/1172 tok/s;   3053 sec
[2019-08-24 10:18:50,465 INFO] Step 850/200000; acc:  95.64; ppl:  1.20; xent: 0.18; lr: 0.00001; 3784/1157 tok/s;   3262 sec
[2019-08-24 10:21:58,470 INFO] Step 900/200000; acc:  95.77; ppl:  1.12; xent: 0.12; lr: 0.00001; 3959/1126 tok/s;   3450 sec
[2019-08-24 10:25:16,581 INFO] Step 950/200000; acc:  96.18; ppl:  1.11; xent: 0.11; lr: 0.00001; 3825/1153 tok/s;   3649 sec
[2019-08-24 10:28:26,632 INFO] Step 1000/200000; acc:  96.11; ppl:  1.11; xent: 0.11; lr: 0.00001; 3903/1170 tok/s;   3839 sec
[2019-08-24 10:31:43,165 INFO] Step 1050/200000; acc:  96.03; ppl:  1.13; xent: 0.12; lr: 0.00001; 3935/1080 tok/s;   4035 sec
[2019-08-24 10:34:49,159 INFO] Step 1100/200000; acc:  96.14; ppl:  1.11; xent: 0.10; lr: 0.00001; 3967/1130 tok/s;   4221 sec
[2019-08-24 10:38:22,281 INFO] Step 1150/200000; acc:  96.35; ppl:  1.12; xent: 0.12; lr: 0.00001; 3757/1144 tok/s;   4434 sec
[2019-08-24 10:41:42,832 INFO] Step 1200/200000; acc:  96.29; ppl:  1.12; xent: 0.12; lr: 0.00001; 3865/1137 tok/s;   4635 sec
[2019-08-24 10:44:54,931 INFO] Step 1250/200000; acc:  96.35; ppl:  1.13; xent: 0.12; lr: 0.00001; 3951/1146 tok/s;   4827 sec
[2019-08-24 10:48:01,480 INFO] Step 1300/200000; acc:  96.28; ppl:  1.11; xent: 0.10; lr: 0.00001; 4014/1116 tok/s;   5013 sec
[2019-08-24 10:51:11,657 INFO] Step 1350/200000; acc:  96.49; ppl:  1.10; xent: 0.10; lr: 0.00001; 3871/1137 tok/s;   5204 sec
[2019-08-24 10:54:31,922 INFO] Step 1400/200000; acc:  96.56; ppl:  1.10; xent: 0.09; lr: 0.00001; 3853/1128 tok/s;   5404 sec
[2019-08-24 10:57:42,966 INFO] Step 1450/200000; acc:  96.62; ppl:  1.10; xent: 0.09; lr: 0.00001; 3892/1172 tok/s;   5595 sec
[2019-08-24 11:00:56,550 INFO] Step 1500/200000; acc:  96.70; ppl:  1.09; xent: 0.09; lr: 0.00001; 3896/1143 tok/s;   5789 sec
[2019-08-24 11:04:13,999 INFO] Step 1550/200000; acc:  96.80; ppl:  1.09; xent: 0.09; lr: 0.00001; 3852/1169 tok/s;   5986 sec
[2019-08-24 11:05:04,957 INFO] Loading dataset from /home/LAB/zhangzy/ProjectData/openNMT/answer_data/xlnetGen/data.train.1.pt
[2019-08-24 11:05:42,944 INFO] number of examples: 100000
[2019-08-24 11:07:59,137 INFO] Step 1600/200000; acc:  96.72; ppl:  1.09; xent: 0.09; lr: 0.00001; 3198/953 tok/s;   6211 sec
[2019-08-24 11:10:55,795 INFO] Step 1650/200000; acc:  96.68; ppl:  1.09; xent: 0.09; lr: 0.00001; 3945/1179 tok/s;   6388 sec
[2019-08-24 11:13:41,692 INFO] Step 1700/200000; acc:  96.73; ppl:  1.09; xent: 0.09; lr: 0.00001; 4009/1191 tok/s;   6554 sec
[2019-08-24 11:16:32,119 INFO] Step 1750/200000; acc:  96.79; ppl:  1.09; xent: 0.09; lr: 0.00001; 4063/1221 tok/s;   6724 sec
[2019-08-24 11:19:36,005 INFO] Step 1800/200000; acc:  96.81; ppl:  1.10; xent: 0.09; lr: 0.00001; 3930/1147 tok/s;   6908 sec
[2019-08-24 11:22:39,158 INFO] Step 1850/200000; acc:  96.82; ppl:  1.10; xent: 0.10; lr: 0.00001; 3894/1185 tok/s;   7091 sec
[2019-08-24 11:25:41,840 INFO] Step 1900/200000; acc:  96.70; ppl:  1.09; xent: 0.09; lr: 0.00001; 4045/1109 tok/s;   7274 sec
[2019-08-24 11:28:45,000 INFO] Step 1950/200000; acc:  96.98; ppl:  1.09; xent: 0.08; lr: 0.00001; 3831/1187 tok/s;   7457 sec
[2019-08-24 11:31:46,730 INFO] Step 2000/200000; acc:  96.97; ppl:  1.11; xent: 0.11; lr: 0.00001; 3898/1183 tok/s;   7639 sec
[2019-08-24 11:34:44,208 INFO] Step 2050/200000; acc:  96.97; ppl:  1.09; xent: 0.08; lr: 0.00001; 4005/1193 tok/s;   7816 sec
[2019-08-24 11:37:46,398 INFO] Step 2100/200000; acc:  97.05; ppl:  1.08; xent: 0.08; lr: 0.00001; 3952/1152 tok/s;   7998 sec
[2019-08-24 11:40:31,398 INFO] Step 2150/200000; acc:  96.82; ppl:  1.09; xent: 0.08; lr: 0.00001; 4076/1174 tok/s;   8163 sec
[2019-08-24 11:43:26,021 INFO] Step 2200/200000; acc:  96.97; ppl:  1.08; xent: 0.08; lr: 0.00001; 4020/1164 tok/s;   8338 sec
[2019-08-24 11:46:18,926 INFO] Step 2250/200000; acc:  97.17; ppl:  1.08; xent: 0.08; lr: 0.00001; 4011/1210 tok/s;   8511 sec
[2019-08-24 11:49:14,923 INFO] Step 2300/200000; acc:  97.05; ppl:  1.08; xent: 0.08; lr: 0.00001; 3989/1174 tok/s;   8687 sec
[2019-08-24 11:52:21,224 INFO] Step 2350/200000; acc:  97.13; ppl:  1.08; xent: 0.08; lr: 0.00001; 3914/1147 tok/s;   8873 sec
[2019-08-24 11:55:24,523 INFO] Step 2400/200000; acc:  97.13; ppl:  1.08; xent: 0.08; lr: 0.00001; 3936/1130 tok/s;   9057 sec
[2019-08-24 11:58:16,480 INFO] Step 2450/200000; acc:  97.08; ppl:  1.08; xent: 0.08; lr: 0.00001; 4002/1164 tok/s;   9228 sec
[2019-08-24 12:01:12,568 INFO] Step 2500/200000; acc:  96.98; ppl:  1.08; xent: 0.08; lr: 0.00001; 3916/1146 tok/s;   9405 sec
[2019-08-24 12:04:06,323 INFO] Step 2550/200000; acc:  97.08; ppl:  1.08; xent: 0.08; lr: 0.00001; 4006/1154 tok/s;   9578 sec
[2019-08-24 12:07:09,290 INFO] Step 2600/200000; acc:  97.10; ppl:  1.09; xent: 0.09; lr: 0.00001; 3868/1145 tok/s;   9761 sec
[2019-08-24 12:10:06,145 INFO] Step 2650/200000; acc:  97.28; ppl:  1.08; xent: 0.07; lr: 0.00001; 3893/1208 tok/s;   9938 sec
[2019-08-24 12:13:14,393 INFO] Step 2700/200000; acc:  97.31; ppl:  1.08; xent: 0.07; lr: 0.00001; 3892/1165 tok/s;  10126 sec
[2019-08-24 12:16:20,666 INFO] Step 2750/200000; acc:  97.11; ppl:  1.09; xent: 0.09; lr: 0.00001; 3858/1161 tok/s;  10313 sec
[2019-08-24 12:19:15,368 INFO] Step 2800/200000; acc:  97.30; ppl:  1.07; xent: 0.07; lr: 0.00001; 3986/1183 tok/s;  10487 sec
[2019-08-24 12:22:05,973 INFO] Step 2850/200000; acc:  97.22; ppl:  1.08; xent: 0.07; lr: 0.00001; 3965/1184 tok/s;  10658 sec
[2019-08-24 12:25:00,182 INFO] Step 2900/200000; acc:  97.15; ppl:  1.08; xent: 0.07; lr: 0.00001; 3923/1127 tok/s;  10832 sec
[2019-08-24 12:28:05,041 INFO] Step 2950/200000; acc:  97.23; ppl:  1.08; xent: 0.07; lr: 0.00001; 3861/1149 tok/s;  11017 sec
[2019-08-24 12:31:02,320 INFO] Step 3000/200000; acc:  97.31; ppl:  1.07; xent: 0.07; lr: 0.00001; 3923/1168 tok/s;  11194 sec
[2019-08-24 12:33:57,659 INFO] Step 3050/200000; acc:  97.26; ppl:  1.08; xent: 0.07; lr: 0.00001; 3931/1153 tok/s;  11370 sec
[2019-08-24 12:36:58,386 INFO] Step 3100/200000; acc:  97.40; ppl:  1.07; xent: 0.07; lr: 0.00001; 3934/1180 tok/s;  11550 sec
[2019-08-24 12:38:25,881 INFO] Loading dataset from /home/LAB/zhangzy/ProjectData/openNMT/answer_data/xlnetGen/data.train.10.pt
[2019-08-24 12:38:59,421 INFO] number of examples: 100000
[2019-08-24 12:40:17,233 INFO] Step 3150/200000; acc:  97.24; ppl:  1.08; xent: 0.07; lr: 0.00001; 3269/985 tok/s;  11749 sec
[2019-08-24 12:42:42,883 INFO] Step 3200/200000; acc:  97.26; ppl:  1.08; xent: 0.07; lr: 0.00001; 4018/1248 tok/s;  11895 sec
[2019-08-24 12:45:01,876 INFO] Step 3250/200000; acc:  97.22; ppl:  1.08; xent: 0.07; lr: 0.00001; 4052/1258 tok/s;  12034 sec
[2019-08-24 12:47:27,117 INFO] Step 3300/200000; acc:  97.15; ppl:  1.08; xent: 0.08; lr: 0.00001; 4125/1240 tok/s;  12179 sec
[2019-08-24 12:49:53,748 INFO] Step 3350/200000; acc:  97.22; ppl:  1.08; xent: 0.08; lr: 0.00001; 4035/1215 tok/s;  12326 sec
[2019-08-24 12:52:32,377 INFO] Step 3400/200000; acc:  96.54; ppl:  1.18; xent: 0.16; lr: 0.00001; 3977/1184 tok/s;  12484 sec
[2019-08-24 12:55:04,667 INFO] Step 3450/200000; acc:  97.23; ppl:  1.08; xent: 0.07; lr: 0.00001; 4086/1210 tok/s;  12637 sec
[2019-08-24 12:57:39,377 INFO] Step 3500/200000; acc:  97.29; ppl:  1.07; xent: 0.07; lr: 0.00001; 4021/1214 tok/s;  12791 sec
[2019-08-24 12:59:56,214 INFO] Step 3550/200000; acc:  97.05; ppl:  1.08; xent: 0.08; lr: 0.00001; 4193/1204 tok/s;  12928 sec
[2019-08-24 13:02:29,128 INFO] Step 3600/200000; acc:  97.30; ppl:  1.08; xent: 0.07; lr: 0.00001; 4005/1204 tok/s;  13081 sec
[2019-08-24 13:05:08,214 INFO] Step 3650/200000; acc:  97.34; ppl:  1.07; xent: 0.07; lr: 0.00001; 3934/1205 tok/s;  13240 sec
[2019-08-24 13:07:33,652 INFO] Step 3700/200000; acc:  97.39; ppl:  1.07; xent: 0.07; lr: 0.00001; 4012/1265 tok/s;  13386 sec
[2019-08-24 13:09:57,611 INFO] Step 3750/200000; acc:  97.34; ppl:  1.07; xent: 0.07; lr: 0.00001; 4124/1246 tok/s;  13530 sec
[2019-08-24 13:12:24,156 INFO] Step 3800/200000; acc:  97.35; ppl:  1.07; xent: 0.07; lr: 0.00001; 4017/1247 tok/s;  13676 sec
[2019-08-24 13:14:56,211 INFO] Step 3850/200000; acc:  97.47; ppl:  1.07; xent: 0.07; lr: 0.00001; 3941/1238 tok/s;  13828 sec
[2019-08-24 13:17:26,582 INFO] Step 3900/200000; acc:  97.27; ppl:  1.07; xent: 0.07; lr: 0.00001; 4056/1237 tok/s;  13979 sec
[2019-08-24 13:19:56,731 INFO] Step 3950/200000; acc:  97.41; ppl:  1.07; xent: 0.07; lr: 0.00001; 4002/1214 tok/s;  14129 sec
[2019-08-24 13:22:26,165 INFO] Step 4000/200000; acc:  97.25; ppl:  1.07; xent: 0.07; lr: 0.00001; 4126/1179 tok/s;  14278 sec
[2019-08-24 13:24:45,839 INFO] Step 4050/200000; acc:  97.32; ppl:  1.07; xent: 0.07; lr: 0.00001; 4126/1253 tok/s;  14418 sec
[2019-08-24 13:27:09,754 INFO] Step 4100/200000; acc:  97.42; ppl:  1.07; xent: 0.07; lr: 0.00001; 4085/1230 tok/s;  14562 sec
[2019-08-24 13:29:44,340 INFO] Step 4150/200000; acc:  97.22; ppl:  1.11; xent: 0.11; lr: 0.00001; 4025/1197 tok/s;  14716 sec
[2019-08-24 13:32:05,592 INFO] Step 4200/200000; acc:  97.44; ppl:  1.07; xent: 0.07; lr: 0.00001; 4097/1245 tok/s;  14858 sec
[2019-08-24 13:34:35,258 INFO] Step 4250/200000; acc:  97.33; ppl:  1.07; xent: 0.07; lr: 0.00001; 4040/1192 tok/s;  15007 sec
[2019-08-24 13:37:10,788 INFO] Step 4300/200000; acc:  97.45; ppl:  1.07; xent: 0.07; lr: 0.00001; 4032/1216 tok/s;  15163 sec
[2019-08-24 13:39:39,762 INFO] Step 4350/200000; acc:  97.55; ppl:  1.07; xent: 0.06; lr: 0.00001; 3958/1265 tok/s;  15312 sec
[2019-08-24 13:41:58,424 INFO] Step 4400/200000; acc:  97.28; ppl:  1.07; xent: 0.07; lr: 0.00001; 4182/1189 tok/s;  15450 sec
[2019-08-24 13:44:31,028 INFO] Step 4450/200000; acc:  97.41; ppl:  1.07; xent: 0.07; lr: 0.00001; 4007/1204 tok/s;  15603 sec
[2019-08-24 13:47:04,086 INFO] Step 4500/200000; acc:  97.50; ppl:  1.07; xent: 0.07; lr: 0.00001; 3911/1246 tok/s;  15756 sec
[2019-08-24 13:49:23,484 INFO] Step 4550/200000; acc:  97.38; ppl:  1.07; xent: 0.07; lr: 0.00001; 4135/1230 tok/s;  15895 sec
[2019-08-24 13:51:51,722 INFO] Step 4600/200000; acc:  97.64; ppl:  1.06; xent: 0.06; lr: 0.00001; 4009/1276 tok/s;  16044 sec
[2019-08-24 13:54:16,316 INFO] Step 4650/200000; acc:  97.45; ppl:  1.07; xent: 0.07; lr: 0.00001; 4097/1215 tok/s;  16188 sec
[2019-08-24 13:56:08,476 INFO] Loading dataset from /home/LAB/zhangzy/ProjectData/openNMT/answer_data/xlnetGen/data.train.11.pt
[2019-08-24 13:56:41,190 INFO] number of examples: 100000
[2019-08-24 13:57:24,828 INFO] Step 4700/200000; acc:  97.45; ppl:  1.07; xent: 0.07; lr: 0.00001; 3262/1005 tok/s;  16377 sec
[2019-08-24 13:59:44,693 INFO] Step 4750/200000; acc:  97.41; ppl:  1.07; xent: 0.07; lr: 0.00001; 4096/1207 tok/s;  16517 sec
[2019-08-24 14:02:03,478 INFO] Step 4800/200000; acc:  97.46; ppl:  1.07; xent: 0.07; lr: 0.00001; 4141/1230 tok/s;  16655 sec
[2019-08-24 14:04:30,241 INFO] Step 4850/200000; acc:  97.46; ppl:  1.07; xent: 0.07; lr: 0.00001; 4017/1227 tok/s;  16802 sec
[2019-08-24 14:06:53,842 INFO] Step 4900/200000; acc:  97.53; ppl:  1.07; xent: 0.06; lr: 0.00001; 4055/1253 tok/s;  16946 sec
[2019-08-24 14:09:27,898 INFO] Step 4950/200000; acc:  97.38; ppl:  1.10; xent: 0.10; lr: 0.00001; 3989/1200 tok/s;  17100 sec
[2019-08-24 14:11:57,106 INFO] Step 5000/200000; acc:  97.43; ppl:  1.09; xent: 0.08; lr: 0.00001; 4015/1281 tok/s;  17249 sec
[2019-08-24 14:14:31,754 INFO] Step 5050/200000; acc:  97.46; ppl:  1.07; xent: 0.07; lr: 0.00001; 4061/1197 tok/s;  17404 sec
[2019-08-24 14:16:51,674 INFO] Step 5100/200000; acc:  97.57; ppl:  1.07; xent: 0.06; lr: 0.00001; 4093/1273 tok/s;  17544 sec
[2019-08-24 14:19:29,065 INFO] Step 5150/200000; acc:  97.68; ppl:  1.06; xent: 0.06; lr: 0.00001; 3920/1213 tok/s;  17701 sec
[2019-08-24 14:22:03,090 INFO] Step 5200/200000; acc:  97.56; ppl:  1.07; xent: 0.06; lr: 0.00001; 3993/1190 tok/s;  17855 sec
[2019-08-24 14:24:28,771 INFO] Step 5250/200000; acc:  97.61; ppl:  1.06; xent: 0.06; lr: 0.00001; 4006/1252 tok/s;  18001 sec
[2019-08-24 14:26:49,064 INFO] Step 5300/200000; acc:  97.47; ppl:  1.07; xent: 0.06; lr: 0.00001; 4101/1232 tok/s;  18141 sec
[2019-08-24 14:29:17,581 INFO] Step 5350/200000; acc:  97.69; ppl:  1.06; xent: 0.06; lr: 0.00001; 3974/1262 tok/s;  18290 sec
[2019-08-24 14:31:39,990 INFO] Step 5400/200000; acc:  97.55; ppl:  1.06; xent: 0.06; lr: 0.00001; 4117/1254 tok/s;  18432 sec
[2019-08-24 14:34:10,581 INFO] Step 5450/200000; acc:  97.74; ppl:  1.06; xent: 0.06; lr: 0.00001; 4022/1259 tok/s;  18583 sec
[2019-08-24 14:36:38,043 INFO] Step 5500/200000; acc:  97.53; ppl:  1.07; xent: 0.06; lr: 0.00001; 4021/1194 tok/s;  18730 sec
[2019-08-24 14:39:12,465 INFO] Step 5550/200000; acc:  97.59; ppl:  1.06; xent: 0.06; lr: 0.00001; 4012/1192 tok/s;  18884 sec
[2019-08-24 14:41:32,530 INFO] Step 5600/200000; acc:  97.55; ppl:  1.07; xent: 0.06; lr: 0.00001; 4092/1234 tok/s;  19025 sec
[2019-08-24 14:43:57,149 INFO] Step 5650/200000; acc:  97.52; ppl:  1.07; xent: 0.06; lr: 0.00001; 4051/1208 tok/s;  19169 sec
[2019-08-24 14:46:26,242 INFO] Step 5700/200000; acc:  97.64; ppl:  1.08; xent: 0.08; lr: 0.00001; 4038/1223 tok/s;  19318 sec
[2019-08-24 14:48:54,456 INFO] Step 5750/200000; acc:  97.75; ppl:  1.06; xent: 0.06; lr: 0.00001; 3938/1264 tok/s;  19466 sec
[2019-08-24 14:51:18,707 INFO] Step 5800/200000; acc:  97.58; ppl:  1.06; xent: 0.06; lr: 0.00001; 4093/1217 tok/s;  19611 sec
[2019-08-24 14:53:56,068 INFO] Step 5850/200000; acc:  97.71; ppl:  1.06; xent: 0.06; lr: 0.00001; 3963/1196 tok/s;  19768 sec
[2019-08-24 14:56:24,326 INFO] Step 5900/200000; acc:  97.58; ppl:  1.07; xent: 0.06; lr: 0.00001; 4060/1210 tok/s;  19916 sec
[2019-08-24 14:58:40,501 INFO] Step 5950/200000; acc:  97.52; ppl:  1.07; xent: 0.06; lr: 0.00001; 4140/1220 tok/s;  20052 sec
[2019-08-24 15:01:06,208 INFO] Step 6000/200000; acc:  97.57; ppl:  1.06; xent: 0.06; lr: 0.00001; 4112/1213 tok/s;  20198 sec
[2019-08-24 15:03:30,031 INFO] Step 6050/200000; acc:  97.65; ppl:  1.06; xent: 0.06; lr: 0.00001; 4052/1241 tok/s;  20342 sec
[2019-08-24 15:05:56,460 INFO] Step 6100/200000; acc:  97.59; ppl:  1.06; xent: 0.06; lr: 0.00001; 4038/1218 tok/s;  20488 sec
[2019-08-24 15:08:21,845 INFO] Step 6150/200000; acc:  97.80; ppl:  1.06; xent: 0.06; lr: 0.00001; 3985/1273 tok/s;  20634 sec
[2019-08-24 15:10:49,195 INFO] Step 6200/200000; acc:  97.78; ppl:  1.06; xent: 0.06; lr: 0.00001; 3974/1246 tok/s;  20781 sec
[2019-08-24 15:13:14,369 INFO] Step 6250/200000; acc:  97.61; ppl:  1.06; xent: 0.06; lr: 0.00001; 4141/1195 tok/s;  20926 sec
[2019-08-24 15:13:14,524 INFO] Loading dataset from /home/LAB/zhangzy/ProjectData/openNMT/answer_data/xlnetGen/data.train.12.pt
[2019-08-24 15:13:49,855 INFO] number of examples: 100000
[2019-08-24 15:16:12,747 INFO] Step 6300/200000; acc:  97.71; ppl:  1.06; xent: 0.06; lr: 0.00001; 3221/973 tok/s;  21105 sec
[2019-08-24 15:18:38,349 INFO] Step 6350/200000; acc:  97.88; ppl:  1.06; xent: 0.06; lr: 0.00001; 3956/1290 tok/s;  21250 sec
[2019-08-24 15:20:59,565 INFO] Step 6400/200000; acc:  97.60; ppl:  1.06; xent: 0.06; lr: 0.00001; 4068/1259 tok/s;  21392 sec
[2019-08-24 15:23:18,168 INFO] Step 6450/200000; acc:  97.52; ppl:  1.06; xent: 0.06; lr: 0.00001; 4122/1196 tok/s;  21530 sec
[2019-08-24 15:25:48,683 INFO] Step 6500/200000; acc:  97.67; ppl:  1.08; xent: 0.08; lr: 0.00001; 4018/1208 tok/s;  21681 sec
[2019-08-24 15:28:15,010 INFO] Step 6550/200000; acc:  97.07; ppl:  1.10; xent: 0.09; lr: 0.00001; 4041/1192 tok/s;  21827 sec
[2019-08-24 15:30:48,657 INFO] Step 6600/200000; acc:  97.73; ppl:  1.06; xent: 0.06; lr: 0.00001; 3979/1214 tok/s;  21981 sec
[2019-08-24 15:33:11,400 INFO] Step 6650/200000; acc:  97.78; ppl:  1.06; xent: 0.06; lr: 0.00001; 4034/1267 tok/s;  22123 sec
[2019-08-24 15:35:44,875 INFO] Step 6700/200000; acc:  97.77; ppl:  1.06; xent: 0.06; lr: 0.00001; 3954/1190 tok/s;  22277 sec
[2019-08-24 15:38:10,912 INFO] Step 6750/200000; acc:  97.59; ppl:  1.08; xent: 0.07; lr: 0.00001; 4009/1203 tok/s;  22423 sec
[2019-08-24 15:40:41,090 INFO] Step 6800/200000; acc:  97.80; ppl:  1.06; xent: 0.06; lr: 0.00001; 4001/1196 tok/s;  22573 sec
[2019-08-24 15:42:51,552 INFO] Step 6850/200000; acc:  97.70; ppl:  1.06; xent: 0.06; lr: 0.00001; 4175/1258 tok/s;  22704 sec
[2019-08-24 15:45:17,367 INFO] Step 6900/200000; acc:  97.68; ppl:  1.06; xent: 0.06; lr: 0.00001; 4095/1197 tok/s;  22849 sec
[2019-08-24 15:47:33,669 INFO] Step 6950/200000; acc:  97.62; ppl:  1.06; xent: 0.06; lr: 0.00001; 4218/1204 tok/s;  22986 sec
[2019-08-24 15:50:02,948 INFO] Step 7000/200000; acc:  97.75; ppl:  1.06; xent: 0.06; lr: 0.00001; 3973/1238 tok/s;  23135 sec
[2019-08-24 15:52:31,287 INFO] Step 7050/200000; acc:  97.87; ppl:  1.06; xent: 0.06; lr: 0.00001; 3997/1242 tok/s;  23283 sec
[2019-08-24 15:55:08,730 INFO] Step 7100/200000; acc:  97.93; ppl:  1.05; xent: 0.05; lr: 0.00001; 3883/1233 tok/s;  23441 sec
[2019-08-24 15:57:30,392 INFO] Step 7150/200000; acc:  97.83; ppl:  1.06; xent: 0.06; lr: 0.00001; 4011/1268 tok/s;  23582 sec
[2019-08-24 15:59:55,473 INFO] Step 7200/200000; acc:  97.87; ppl:  1.06; xent: 0.06; lr: 0.00001; 4007/1246 tok/s;  23727 sec
[2019-08-24 16:02:13,376 INFO] Step 7250/200000; acc:  97.72; ppl:  1.06; xent: 0.06; lr: 0.00001; 4109/1250 tok/s;  23865 sec
[2019-08-24 16:04:40,705 INFO] Step 7300/200000; acc:  97.64; ppl:  1.10; xent: 0.10; lr: 0.00001; 4044/1209 tok/s;  24013 sec
[2019-08-24 16:07:01,392 INFO] Step 7350/200000; acc:  97.74; ppl:  1.06; xent: 0.06; lr: 0.00001; 4016/1227 tok/s;  24153 sec
[2019-08-24 16:09:39,865 INFO] Step 7400/200000; acc:  97.85; ppl:  1.06; xent: 0.06; lr: 0.00001; 3909/1204 tok/s;  24312 sec
[2019-08-24 16:12:13,866 INFO] Step 7450/200000; acc:  98.05; ppl:  1.05; xent: 0.05; lr: 0.00001; 3873/1272 tok/s;  24466 sec
[2019-08-24 16:14:37,102 INFO] Step 7500/200000; acc:  97.80; ppl:  1.06; xent: 0.06; lr: 0.00001; 4060/1230 tok/s;  24609 sec
[2019-08-24 16:16:57,584 INFO] Step 7550/200000; acc:  97.71; ppl:  1.06; xent: 0.06; lr: 0.00001; 4075/1226 tok/s;  24750 sec
[2019-08-24 16:19:20,750 INFO] Step 7600/200000; acc:  97.76; ppl:  1.06; xent: 0.06; lr: 0.00001; 3938/1218 tok/s;  24893 sec
[2019-08-24 16:21:50,891 INFO] Step 7650/200000; acc:  97.95; ppl:  1.05; xent: 0.05; lr: 0.00001; 3947/1237 tok/s;  25043 sec
[2019-08-24 16:24:13,790 INFO] Step 7700/200000; acc:  97.90; ppl:  1.06; xent: 0.06; lr: 0.00001; 3986/1273 tok/s;  25186 sec
[2019-08-24 16:26:37,647 INFO] Step 7750/200000; acc:  97.82; ppl:  1.06; xent: 0.06; lr: 0.00001; 4018/1249 tok/s;  25330 sec
[2019-08-24 16:29:01,002 INFO] Step 7800/200000; acc:  97.83; ppl:  1.06; xent: 0.06; lr: 0.00001; 4060/1250 tok/s;  25473 sec
[2019-08-24 16:29:37,330 INFO] Loading dataset from /home/LAB/zhangzy/ProjectData/openNMT/answer_data/xlnetGen/data.train.13.pt
[2019-08-24 16:30:14,153 INFO] number of examples: 100000
[2019-08-24 16:32:05,654 INFO] Step 7850/200000; acc:  97.94; ppl:  1.05; xent: 0.05; lr: 0.00001; 3183/1007 tok/s;  25658 sec
[2019-08-24 16:34:30,428 INFO] Step 7900/200000; acc:  97.85; ppl:  1.06; xent: 0.06; lr: 0.00001; 4045/1187 tok/s;  25802 sec
[2019-08-24 16:36:46,044 INFO] Step 7950/200000; acc:  97.89; ppl:  1.06; xent: 0.05; lr: 0.00001; 4102/1266 tok/s;  25938 sec
[2019-08-24 16:39:05,254 INFO] Step 8000/200000; acc:  97.74; ppl:  1.06; xent: 0.06; lr: 0.00001; 4163/1210 tok/s;  26077 sec
[2019-08-24 16:41:41,335 INFO] Step 8050/200000; acc:  97.92; ppl:  1.07; xent: 0.06; lr: 0.00001; 3900/1253 tok/s;  26233 sec
[2019-08-24 16:44:10,094 INFO] Step 8100/200000; acc:  97.17; ppl:  1.10; xent: 0.09; lr: 0.00001; 4025/1252 tok/s;  26382 sec
[2019-08-24 16:46:45,037 INFO] Step 8150/200000; acc:  97.96; ppl:  1.05; xent: 0.05; lr: 0.00001; 3995/1260 tok/s;  26537 sec
[2019-08-24 16:49:10,048 INFO] Step 8200/200000; acc:  97.91; ppl:  1.05; xent: 0.05; lr: 0.00001; 4057/1222 tok/s;  26682 sec
